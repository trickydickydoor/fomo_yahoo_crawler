#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Yahoo Finance News çˆ¬è™«
ä½¿ç”¨ Crawl4AI å¤„ç†åŠ¨æ€å†…å®¹å’Œæ»šåŠ¨åŠ è½½
"""

import asyncio
import time
import json
import csv
import re
import os
import random
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
from asyncio import Semaphore

# Web scraping
import requests
from bs4 import BeautifulSoup

# Crawl4AI
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, VirtualScrollConfig

# Supabase
from supabase_manager import SupabaseManager, create_supabase_manager

# å°è¯•åŠ è½½.envæ–‡ä»¶ï¼ˆæœ¬åœ°å¼€å‘ä½¿ç”¨ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # GitHub Actionsç¯å¢ƒä¸éœ€è¦dotenv
    pass


class AntiDetection:
    """ååçˆ¬è™«å·¥å…·ç±»"""
    
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    
    @staticmethod
    def get_random_headers():
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': random.choice(AntiDetection.USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
    
    @staticmethod
    async def random_delay(min_seconds=1.0, max_seconds=3.0):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(min_seconds, max_seconds)
        await asyncio.sleep(delay)


class YahooNewsCrawl4AICrawler:
    """åŸºäºCrawl4AIçš„Yahoo Financeæ–°é—»çˆ¬è™« - æ”¯æŒå¤šURLçˆ¬å–"""
    
    def __init__(self, supabase_config=None):
        self.articles = []
        self.urls = [
            "https://finance.yahoo.com/topic/latest-news/",
            "https://finance.yahoo.com/news/", 
            "https://finance.yahoo.com/topic/tech/"
        ]
        self.is_ci_environment = self._detect_ci_environment()
        
        # åˆå§‹åŒ–Supabaseç®¡ç†å™¨
        self.supabase_manager = None
        if supabase_config:
            self.supabase_manager = create_supabase_manager(supabase_config)
            if self.supabase_manager:
                print("ğŸ—„ï¸ Supabaseæ•°æ®åº“é›†æˆå·²å¯ç”¨")
            else:
                print("âš ï¸ Supabaseè¿æ¥å¤±è´¥ï¼Œå°†åªä¿å­˜æœ¬åœ°æ–‡ä»¶")
        
    def _detect_ci_environment(self):
        """æ£€æµ‹æ˜¯å¦åœ¨CI/CDç¯å¢ƒä¸­è¿è¡Œ"""
        ci_indicators = [
            'GITHUB_ACTIONS',
            'CI', 
            'CONTINUOUS_INTEGRATION',
            'GITLAB_CI',
            'JENKINS_URL'
        ]
        return any(os.getenv(indicator) for indicator in ci_indicators)
        
    def is_within_hours(self, time_text, max_hours=2):
        """æ£€æŸ¥æ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šå°æ—¶å†…"""
        if not time_text:
            return True
        
        time_text = str(time_text).lower().strip()
        
        # åŒ¹é…"Xåˆ†é’Ÿå‰" æˆ– "X minutes ago"
        if 'minute' in time_text or 'åˆ†é’Ÿ' in time_text:
            return True
        
        # åŒ¹é…"Xå°æ—¶å‰" æˆ– "X hours ago"  
        if 'hour' in time_text or 'å°æ—¶' in time_text:
            hours_match = re.search(r'(\d+)', time_text)
            if hours_match:
                hours = int(hours_match.group(1))
                return hours <= max_hours
            return True  # "an hour ago" æˆ– "1å°æ—¶å‰"
        
        # åŒ¹é…"åˆšåˆš"ã€"just now"ç­‰
        if any(word in time_text for word in ['just now', 'now', 'åˆšåˆš', 'åˆšæ‰']):
            return True
            
        # å¦‚æœæ— æ³•è§£æï¼Œé»˜è®¤åŒ…å«ï¼ˆå‡è®¾æ˜¯æœ€æ–°çš„ï¼‰
        return True
    
    async def get_article_content(self, article_url):
        """è·å–æ–‡ç« çš„å®Œæ•´å†…å®¹å’Œå‡†ç¡®æ—¶é—´ - ä½¿ç”¨requestsæ›¿ä»£Crawl4AI"""
        try:
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            await AntiDetection.random_delay(0.5, 2.0)
            
            # ä½¿ç”¨éšæœºè¯·æ±‚å¤´
            headers = AntiDetection.get_random_headers()
            
            # è®¾ç½®ä¼šè¯
            timeout = 15 if self.is_ci_environment else 10
            
            # å‘èµ·è¯·æ±‚
            response = requests.get(article_url, headers=headers, timeout=timeout)
            response.raise_for_status()
            
            # è§£æå†…å®¹
            return self._extract_article_details(response.text, article_url)
            
        except requests.exceptions.Timeout:
            print(f"âš ï¸ è¯·æ±‚è¶…æ—¶: {article_url}")
            return {"content": "", "full_time": ""}
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ {article_url}: {e}")
            return {"content": "", "full_time": ""}
        except Exception as e:
            print(f"âš ï¸ è·å–æ–‡ç« å†…å®¹å‡ºé”™ {article_url}: {e}")
            return {"content": "", "full_time": ""}
    
    def _extract_article_details(self, html_content, article_url):
        """ä»æ–‡ç« é¡µé¢HTMLä¸­æå–å†…å®¹å’Œæ—¶é—´"""
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            
            # æå–æ–‡ç« å†…å®¹
            content_parts = []
            
            # 1. è·å–ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content_selectors = [
                'div[data-testid="caas-body"]',
                '.caas-body',
                'div.caas-body',
                'div[class*="body"]'
            ]
            
            main_content = None
            for selector in main_content_selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if main_content:
                # 2. è·å–å¯è§çš„æ®µè½å†…å®¹
                visible_paragraphs = main_content.find_all('p', class_=lambda x: x and 'yf-' in str(x))
                for p in visible_paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                        content_parts.append(text)
                
                # 3. è·å–éšè—çš„read-moreå†…å®¹
                read_more_wrapper = main_content.find('div', class_='read-more-wrapper')
                if read_more_wrapper:
                    hidden_paragraphs = read_more_wrapper.find_all('p')
                    for p in hidden_paragraphs:
                        # è·å–HTML_TAG_STARTåˆ°HTML_TAG_ENDä¹‹é—´çš„å†…å®¹
                        text = p.get_text(strip=True)
                        if text and len(text) > 10 and 'Read the original article' not in text:
                            content_parts.append(text)
            
            # æå–å‡†ç¡®çš„å‘å¸ƒæ—¶é—´
            full_time = ""
            time_selectors = [
                'time[datetime]',
                '[data-testid="timestamp"]',
                'div[data-testid="caas-attr-time-style"]',
                '.caas-attr-time-style'
            ]
            
            for selector in time_selectors:
                time_element = soup.select_one(selector)
                if time_element:
                    full_time = time_element.get('datetime') or time_element.get_text(strip=True)
                    if full_time:
                        break
            
            # ç»„åˆå®Œæ•´å†…å®¹
            full_content = '\n\n'.join(content_parts) if content_parts else ""
            
            return {
                "content": full_content,
                "full_time": full_time or "Recent"
            }
            
        except Exception as e:
            print(f"âš ï¸ è§£ææ–‡ç« è¯¦æƒ…å‡ºé”™ {article_url}: {e}")
            return {"content": "", "full_time": ""}
    
    
    def parse_html_content(self, html_content, max_hours=2):
        """è§£æHTMLå†…å®¹ï¼Œæå–æ–°é—»æ•°æ®"""
        print("è§£æHTMLå†…å®¹...")
        
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            
            # è°ƒè¯•ï¼šæŸ¥çœ‹é¡µé¢ä¸­æœ‰å“ªäº›ç›¸å…³çš„CSSç±»
            all_lis = soup.find_all('li')
            print(f"é¡µé¢ä¸­æ€»å…±æœ‰ {len(all_lis)} ä¸ªliå…ƒç´ ")
            
            # æŸ¥æ‰¾åŒ…å«"stream"æˆ–"item"çš„liå…ƒç´ 
            stream_items = soup.find_all('li', class_=lambda x: x and ('stream' in str(x) or 'item' in str(x)))
            print(f"æ‰¾åˆ° {len(stream_items)} ä¸ªåŒ…å«streamæˆ–itemçš„liå…ƒç´ ")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªå’Œæœ€åå‡ ä¸ªçš„classåç§°
            for i, item in enumerate(stream_items[:3]):
                classes = item.get('class')
                print(f"  å¼€å§‹liå…ƒç´  {i+1}: class='{classes}'")
            
            if len(stream_items) > 6:
                print("  ...")
                for i, item in enumerate(stream_items[-3:], len(stream_items)-2):
                    classes = item.get('class')
                    print(f"  ç»“å°¾liå…ƒç´  {i}: class='{classes}'")
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–°é—»é¡¹ç›®ï¼ˆæ’é™¤å¹¿å‘Šï¼‰ - ä½¿ç”¨æ›´å®½æ¾çš„åŒ¹é…
            story_items = []
            
            # æ–¹æ³•1ï¼šä¸¥æ ¼åŒ¹é…
            story_items_strict = soup.find_all('li', class_=lambda x: x and 'stream-item' in str(x) and 'story-item' in str(x) and 'ad-item' not in str(x))
            print(f"ä¸¥æ ¼åŒ¹é…æ‰¾åˆ° {len(story_items_strict)} ä¸ªstory-item")
            
            # æ–¹æ³•2ï¼šå®½æ¾åŒ¹é…
            story_items_loose = soup.find_all('li', class_=lambda x: x and 'item' in str(x) and 'ad' not in str(x))
            print(f"å®½æ¾åŒ¹é…æ‰¾åˆ° {len(story_items_loose)} ä¸ªitemï¼ˆéå¹¿å‘Šï¼‰")
            
            # ä½¿ç”¨æœ‰ç»“æœçš„æ–¹æ³•
            if len(story_items_strict) > 0:
                story_items = story_items_strict
                print("ä½¿ç”¨ä¸¥æ ¼åŒ¹é…ç»“æœ")
            elif len(story_items_loose) > 0:
                story_items = story_items_loose
                print("ä½¿ç”¨å®½æ¾åŒ¹é…ç»“æœ")
            else:
                # æ–¹æ³•3ï¼šæœ€å®½æ¾åŒ¹é… - æŸ¥æ‰¾ä»»ä½•åŒ…å«é“¾æ¥çš„li
                story_items = [item for item in all_lis if item.find('a', href=True)]
                print(f"æœ€å®½æ¾åŒ¹é…æ‰¾åˆ° {len(story_items)} ä¸ªåŒ…å«é“¾æ¥çš„liå…ƒç´ ")
            
            found_count = 0
            for item in story_items:  # å¤„ç†æ‰€æœ‰story_items
                try:
                    # ä»aæ ‡ç­¾çš„aria-labelå±æ€§è·å–æ ‡é¢˜
                    title_link = item.find('a', {'aria-label': True})
                    if not title_link:
                        # å¤‡ç”¨ï¼šæŸ¥æ‰¾ä»»ä½•æœ‰hrefçš„aæ ‡ç­¾
                        title_link = item.find('a', href=True)
                        if not title_link:
                            continue
                    
                    # æå–æ ‡é¢˜ - ä¼˜å…ˆä½¿ç”¨aria-label
                    title = title_link.get('aria-label')
                    if not title:
                        title = title_link.get_text(strip=True)
                    
                    if not title or len(title) < 10:
                        continue
                    
                    # æå–é“¾æ¥
                    link = title_link.get('href')
                    if not link:
                        continue
                        
                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    if link.startswith('/'):
                        link = f"https://finance.yahoo.com{link}"
                    elif not link.startswith('http'):
                        continue
                    
                    # æš‚æ—¶ä¸è¿‡æ»¤æ—¶é—´ï¼Œå…ˆè·å–æ•°æ®
                    time_text = 'Recent'
                    
                    # å»é‡æ£€æŸ¥ - åŸºäºtitleå’Œlink
                    if any(existing['link'] == link or existing['title'] == title for existing in self.articles):
                        continue
                    
                    article = {
                        'title': title,
                        'link': link, 
                        'time': time_text,
                        'source': 'Yahoo Finance',
                        'content': '',  # ç¨åå¡«å……
                        'full_time': ''  # ç¨åå¡«å……å‡†ç¡®æ—¶é—´
                    }
                    
                    self.articles.append(article)
                    found_count += 1
                    print(f"[{found_count:2}] {title[:60]}... ({time_text})")
                    
                except Exception as e:
                    print(f"è§£æå•ä¸ªæ–°é—»é¡¹å‡ºé”™: {e}")
                    continue
            
            print(f"\næˆåŠŸè§£æ {len(self.articles)} ç¯‡æœ€è¿‘{max_hours}å°æ—¶çš„æ–°é—»")
            return self.articles
            
        except Exception as e:
            print(f"HTMLè§£æå‡ºé”™: {e}")
            return []
    
    async def crawl_single_url(self, url, max_hours=2):
        """çˆ¬å–å•ä¸ªURL"""
        print(f"çˆ¬å–URL: {url}")
        
        
        try:
            # é…ç½®æµè§ˆå™¨ - é’ˆå¯¹CIç¯å¢ƒä¼˜åŒ–
            browser_config = BrowserConfig(
                browser_type="chromium", 
                headless=True,
                verbose=False,  # CIç¯å¢ƒä¸­å‡å°‘æ—¥å¿—
                extra_args=[
                    "--no-sandbox",  # CIç¯å¢ƒå¿…éœ€
                    "--disable-dev-shm-usage",  # é¿å…å†…å­˜é—®é¢˜
                    "--disable-gpu",  # ç¦ç”¨GPU
                    "--disable-web-security",  # ç¦ç”¨Webå®‰å…¨é™åˆ¶
                    "--disable-features=VizDisplayCompositor"  # å‡å°‘èµ„æºä½¿ç”¨
                ] if self.is_ci_environment else []
            )
            
            # é…ç½®è™šæ‹Ÿæ»šåŠ¨ - ä½¿ç”¨htmlå®¹å™¨ï¼Œ15æ¬¡æ»šåŠ¨
            virtual_scroll_config = VirtualScrollConfig(
                container_selector="html",  # htmlå®¹å™¨æœ€ç¨³å®š
                scroll_count=15,  # å‡å°‘åˆ°15æ¬¡æ»šåŠ¨
                scroll_by="page_height",  # æŒ‰é¡µé¢é«˜åº¦æ»šåŠ¨
                wait_after_scroll=1.5  # ç­‰å¾…1.5ç§’
            )
            
            # é…ç½®çˆ¬å–å‚æ•° - ä¸ç­‰å¾…DOMï¼Œç›´æ¥è·å–å†…å®¹
            crawl_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=45000,  # 45ç§’è¶…æ—¶
                wait_for=None,  # ä¸ç­‰å¾…ç‰¹å®šäº‹ä»¶ï¼Œæ»šåŠ¨å®Œæˆåç›´æ¥è·å–å†…å®¹
                virtual_scroll_config=virtual_scroll_config
            )
            
            # åˆ›å»ºçˆ¬è™«
            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(
                    url=url,
                    config=crawl_config
                )
                
                if result.success:
                    print(f"âœ… {url} - Crawl4AIçˆ¬å–æˆåŠŸ")
                    print(f"é¡µé¢å†…å®¹é•¿åº¦: {len(result.html)} å­—ç¬¦")
                    
                    # è§£æHTMLå†…å®¹
                    articles = self.parse_html_content(result.html, max_hours)
                    return articles
                else:
                    print(f"âŒ {url} - Crawl4AIçˆ¬å–å¤±è´¥: {result.error_message}")
                    return []
                    
        except Exception as e:
            print(f"âŒ {url} - Crawl4AIçˆ¬å–å‡ºé”™: {e}")
            return []

    async def crawl_with_crawl4ai(self, max_hours=2, max_articles=None):
        """ä½¿ç”¨Crawl4AIè™šæ‹Ÿæ»šåŠ¨çˆ¬å–æ‰€æœ‰URLçš„æ–°é—» - æ–°å¢Supabaseé›†æˆæµç¨‹"""
        print("ä½¿ç”¨Crawl4AIè™šæ‹Ÿæ»šåŠ¨çˆ¬å–å¤šä¸ªURL...")
        
        # é˜¶æ®µ1: è·å–æ‰€æœ‰URLçš„åŸºç¡€æ–‡ç« åˆ—è¡¨ï¼ˆtitle, link, timeï¼‰
        print("\n=== é˜¶æ®µ1: è·å–æ–‡ç« åˆ—è¡¨ ===")
        all_articles = []
        
        for url in self.urls:
            try:
                articles = await self.crawl_single_url(url, max_hours)
                if articles:
                    print(f"ä» {url} è·å–åˆ° {len(articles)} ç¯‡æ–°é—»")
                    all_articles.extend(articles)
                else:
                    print(f"ä» {url} æœªè·å–åˆ°æ–°é—»ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
                    fallback_articles = await self.crawl_single_url_fallback(url, max_hours)
                    if fallback_articles:
                        print(f"å¤‡ç”¨æ–¹æ³•ä» {url} è·å–åˆ° {len(fallback_articles)} ç¯‡æ–°é—»")
                        all_articles.extend(fallback_articles)
                        
            except Exception as e:
                print(f"å¤„ç†URL {url} æ—¶å‡ºé”™: {e}")
                continue
        
        # å†…éƒ¨å»é‡ï¼ˆåŸºäºtitleå’Œlinkï¼‰
        self.articles = []
        for article in all_articles:
            if not any(existing['link'] == article['link'] or existing['title'] == article['title'] for existing in self.articles):
                self.articles.append(article)
        
        print(f"\nä»æ‰€æœ‰URLè·å– {len(all_articles)} ç¯‡æ–°é—»ï¼Œå†…éƒ¨å»é‡åä¿ç•™ {len(self.articles)} ç¯‡")
        
        # é˜¶æ®µ2: SupabaseæŸ¥é‡æ£€æŸ¥
        print("\n=== é˜¶æ®µ2: æ•°æ®åº“æŸ¥é‡æ£€æŸ¥ ===")
        new_articles = self.articles  # é»˜è®¤éƒ½æ˜¯æ–°æ–‡ç« 
        
        if self.supabase_manager and self.supabase_manager.is_connected():
            try:
                # è·å–æ•°æ®åº“ä¸­ç°æœ‰çš„URLå’ŒTitle
                existing_urls, existing_titles = self.supabase_manager.get_existing_articles()
                
                # è¿›è¡ŒæŸ¥é‡æ£€æŸ¥
                new_articles = self.supabase_manager.check_duplicates(self.articles, existing_urls, existing_titles)
                
            except Exception as e:
                print(f"âš ï¸ æ•°æ®åº“æŸ¥é‡å¤±è´¥ï¼Œå°†å¤„ç†æ‰€æœ‰æ–‡ç« : {e}")
                new_articles = self.articles
        else:
            print("âš ï¸ æœªè¿æ¥æ•°æ®åº“ï¼Œè·³è¿‡æ•°æ®åº“æŸ¥é‡")
        
        if not new_articles:
            print("âœ… æ²¡æœ‰æ–°æ–‡ç« éœ€è¦å¤„ç†")
            return self.articles
        
        # é˜¶æ®µ3: åªå¯¹æ–°æ–‡ç« è·å–è¯¦ç»†å†…å®¹ï¼ˆä¿æŒ5çº¿ç¨‹å¹¶å‘ï¼‰
        print(f"\n=== é˜¶æ®µ3: è·å–æ–°æ–‡ç« è¯¦ç»†å†…å®¹ ===")
        print(f"éœ€è¦è·å–è¯¦ç»†å†…å®¹çš„æ–°æ–‡ç« : {len(new_articles)} ç¯‡")
        
        # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        articles_to_process = new_articles[:max_articles] if max_articles else new_articles
        if max_articles and len(new_articles) > max_articles:
            print(f"ğŸ“ é™åˆ¶å¤„ç†æ–‡ç« æ•°é‡: {max_articles} ç¯‡ï¼ˆæ€»å…±æœ‰ {len(new_articles)} ç¯‡æ–°æ–‡ç« ï¼‰")
        
        # è·å–è¯¦ç»†å†…å®¹ - é™ä½å¹¶å‘æ•°ä»¥å‡å°‘è¢«æ£€æµ‹çš„é£é™©
        concurrent_limit = 3 if self.is_ci_environment else 5
        await self.fetch_articles_content_for_articles(articles_to_process, max_concurrent=concurrent_limit)
        
        # é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“
        if self.supabase_manager and self.supabase_manager.is_connected():
            print(f"\n=== é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“ ===")
            try:
                # åªä¿å­˜æœ‰å†…å®¹çš„æ–‡ç« 
                articles_with_content = [article for article in articles_to_process if article.get('content', '').strip()]
                
                if articles_with_content:
                    success = self.supabase_manager.insert_articles(articles_with_content)
                    if success:
                        print(f"âœ… æˆåŠŸå°† {len(articles_with_content)} ç¯‡æ–‡ç« ä¿å­˜åˆ°æ•°æ®åº“")
                    else:
                        print(f"âš ï¸ æ•°æ®åº“ä¿å­˜å¤±è´¥")
                else:
                    print("âš ï¸ æ²¡æœ‰åŒ…å«å†…å®¹çš„æ–‡ç« å¯ä»¥ä¿å­˜")
                    
            except Exception as e:
                print(f"âŒ æ•°æ®åº“ä¿å­˜è¿‡ç¨‹å‡ºé”™: {e}")
        
        # æ›´æ–°self.articlesä¸ºå¤„ç†è¿‡çš„æ–‡ç« 
        self.articles = articles_to_process
        
        return self.articles
    
    async def fetch_articles_content_for_articles(self, articles, max_concurrent=5):
        """æ‰¹é‡å¹¶å‘è·å–æŒ‡å®šæ–‡ç« åˆ—è¡¨çš„è¯¦ç»†å†…å®¹å’Œæ—¶é—´"""
        if not articles:
            return
        
        total_articles = len(articles)
        print(f"\nå¼€å§‹å¹¶å‘è·å– {total_articles} ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹ï¼ˆ{max_concurrent}å¹¶å‘ï¼‰...")
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
        semaphore = Semaphore(max_concurrent)
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        for i, article in enumerate(articles):
            task = self.fetch_single_article_content(semaphore, i+1, total_articles, article)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        await asyncio.gather(*tasks, return_exceptions=True)
        
        print(f"\næ–‡ç« å†…å®¹è·å–å®Œæˆ!")
    
    async def fetch_articles_content(self, max_articles=None, max_concurrent=5):
        """æ‰¹é‡å¹¶å‘è·å–æ–‡ç« çš„è¯¦ç»†å†…å®¹å’Œæ—¶é—´ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        if not self.articles:
            return
        
        # é™åˆ¶æ–‡ç« æ•°é‡ä»¥é¿å…è¿‡é•¿æ—¶é—´
        articles_to_process = self.articles[:max_articles] if max_articles else self.articles
        await self.fetch_articles_content_for_articles(articles_to_process, max_concurrent)
    
    async def fetch_single_article_content(self, semaphore, index, total, article):
        """è·å–å•ç¯‡æ–‡ç« å†…å®¹ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰"""
        async with semaphore:  # é™åˆ¶å¹¶å‘æ•°é‡
            try:
                print(f"[{index:2}/{total}] è·å–å†…å®¹: {article['title'][:50]}...")
                
                # æ·»åŠ æ€»ä½“è¶…æ—¶ä¿æŠ¤ - requestsæ¯”Crawl4AIå¿«ï¼Œç¼©çŸ­è¶…æ—¶æ—¶é—´
                timeout_limit = 20.0 if self.is_ci_environment else 15.0
                try:
                    details = await asyncio.wait_for(
                        self.get_article_content(article['link']),
                        timeout=timeout_limit
                    )
                except asyncio.TimeoutError:
                    print(f"    âš ï¸ [{index:2}] è·å–è¶…æ—¶ï¼Œè·³è¿‡")
                    details = {"content": "", "full_time": ""}
                
                # æ›´æ–°æ–‡ç« ä¿¡æ¯
                article['content'] = details['content']
                article['full_time'] = details['full_time']
                
                if details['content']:
                    content_length = len(details['content'])
                    print(f"    âœ… [{index:2}] è·å–åˆ° {content_length} å­—ç¬¦å†…å®¹")
                else:
                    print(f"    âš ï¸ [{index:2}] æœªè·å–åˆ°å†…å®¹")
                    
            except Exception as e:
                print(f"    âŒ [{index:2}] è·å–å†…å®¹å¤±è´¥: {e}")
                # ç¡®ä¿å¤±è´¥æ—¶ä¹Ÿæœ‰é»˜è®¤å€¼
                article['content'] = ''
                article['full_time'] = ''
    
    async def crawl_single_url_fallback(self, url, max_hours=2):
        """å•URLå¤‡ç”¨çˆ¬å–æ–¹æ³•ï¼ˆä½¿ç”¨requests + ååçˆ¬è™«ï¼‰"""
        print(f"å¤‡ç”¨æ–¹æ³•çˆ¬å–: {url}")
        
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            await AntiDetection.random_delay(1.0, 3.0)
            
            # ä½¿ç”¨éšæœºè¯·æ±‚å¤´
            headers = AntiDetection.get_random_headers()
            
            timeout = 20 if self.is_ci_environment else 15
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            
            print(f"âœ… {url} - å¤‡ç”¨æ–¹æ³•ï¼šé¡µé¢å†…å®¹é•¿åº¦ {len(response.text)} å­—ç¬¦")
            articles = self.parse_html_content(response.text, max_hours)
            return articles
            
        except Exception as e:
            print(f"âŒ {url} - å¤‡ç”¨çˆ¬å–æ–¹æ³•å¤±è´¥: {e}")
            return []

    async def crawl_fallback(self, max_hours=2, max_articles=None):
        """å¤‡ç”¨çˆ¬å–æ–¹æ³•ï¼ˆä¸ä½¿ç”¨Crawl4AIï¼‰- çˆ¬å–æ‰€æœ‰URLï¼Œé›†æˆSupabaseæµç¨‹"""
        print("ä½¿ç”¨å¤‡ç”¨æ–¹æ³•çˆ¬å–æ‰€æœ‰URL...")
        
        # é˜¶æ®µ1: è·å–æ–‡ç« åˆ—è¡¨
        print("\n=== é˜¶æ®µ1: è·å–æ–‡ç« åˆ—è¡¨ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰===")
        all_articles = []
        
        for url in self.urls:
            try:
                articles = await self.crawl_single_url_fallback(url, max_hours)
                if articles:
                    print(f"å¤‡ç”¨æ–¹æ³•ä» {url} è·å–åˆ° {len(articles)} ç¯‡æ–°é—»")
                    all_articles.extend(articles)
                    
            except Exception as e:
                print(f"å¤‡ç”¨æ–¹æ³•å¤„ç†URL {url} æ—¶å‡ºé”™: {e}")
                continue
        
        # å†…éƒ¨å»é‡
        self.articles = []
        for article in all_articles:
            if not any(existing['link'] == article['link'] or existing['title'] == article['title'] for existing in self.articles):
                self.articles.append(article)
        
        print(f"\nå¤‡ç”¨æ–¹æ³•è·å– {len(all_articles)} ç¯‡æ–°é—»ï¼Œå†…éƒ¨å»é‡åä¿ç•™ {len(self.articles)} ç¯‡")
        
        # é˜¶æ®µ2: SupabaseæŸ¥é‡æ£€æŸ¥
        print("\n=== é˜¶æ®µ2: æ•°æ®åº“æŸ¥é‡æ£€æŸ¥ ===")
        new_articles = self.articles
        
        if self.supabase_manager and self.supabase_manager.is_connected():
            try:
                existing_urls, existing_titles = self.supabase_manager.get_existing_articles()
                new_articles = self.supabase_manager.check_duplicates(self.articles, existing_urls, existing_titles)
            except Exception as e:
                print(f"âš ï¸ æ•°æ®åº“æŸ¥é‡å¤±è´¥ï¼Œå°†å¤„ç†æ‰€æœ‰æ–‡ç« : {e}")
                new_articles = self.articles
        else:
            print("âš ï¸ æœªè¿æ¥æ•°æ®åº“ï¼Œè·³è¿‡æ•°æ®åº“æŸ¥é‡")
        
        if not new_articles:
            print("âœ… æ²¡æœ‰æ–°æ–‡ç« éœ€è¦å¤„ç†")
            return self.articles
        
        # é˜¶æ®µ3: è·å–æ–°æ–‡ç« è¯¦ç»†å†…å®¹
        print(f"\n=== é˜¶æ®µ3: è·å–æ–°æ–‡ç« è¯¦ç»†å†…å®¹ ===")
        articles_to_process = new_articles[:max_articles] if max_articles else new_articles
        if max_articles and len(new_articles) > max_articles:
            print(f"ğŸ“ é™åˆ¶å¤„ç†æ–‡ç« æ•°é‡: {max_articles} ç¯‡ï¼ˆæ€»å…±æœ‰ {len(new_articles)} ç¯‡æ–°æ–‡ç« ï¼‰")
        
        # è·å–è¯¦ç»†å†…å®¹ - é™ä½å¹¶å‘æ•°ä»¥å‡å°‘è¢«æ£€æµ‹çš„é£é™©
        concurrent_limit = 3 if self.is_ci_environment else 5
        await self.fetch_articles_content_for_articles(articles_to_process, max_concurrent=concurrent_limit)
        
        # é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“
        if self.supabase_manager and self.supabase_manager.is_connected():
            print(f"\n=== é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“ ===")
            try:
                articles_with_content = [article for article in articles_to_process if article.get('content', '').strip()]
                if articles_with_content:
                    success = self.supabase_manager.insert_articles(articles_with_content)
                    if success:
                        print(f"âœ… æˆåŠŸå°† {len(articles_with_content)} ç¯‡æ–‡ç« ä¿å­˜åˆ°æ•°æ®åº“")
            except Exception as e:
                print(f"âŒ æ•°æ®åº“ä¿å­˜è¿‡ç¨‹å‡ºé”™: {e}")
        
        self.articles = articles_to_process
        return self.articles
    
    def save_json(self, filename=None):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        if not self.articles:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"yahoo_multi_news_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.articles, f, ensure_ascii=False, indent=2)
        
        print(f"JSONå·²ä¿å­˜: {filename}")
        return filename
    
    def save_csv(self, filename=None):
        """ä¿å­˜ä¸ºCSVæ ¼å¼"""
        if not self.articles:
            return None
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"yahoo_multi_news_{timestamp}.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['title', 'link', 'time', 'source', 'content', 'full_time']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.articles)
        
        print(f"CSVå·²ä¿å­˜: {filename}")
        return filename
    
    def show_summary(self):
        """æ˜¾ç¤ºçˆ¬å–ç»“æœæ‘˜è¦"""
        if not self.articles:
            print("æ²¡æœ‰æ‰¾åˆ°æ–°é—»")
            return
        
        print(f"\n{'='*60}")
        print(f"å¤šURLçˆ¬å–ç»“æœæ±‡æ€»")
        print(f"{'='*60}")
        print(f"æ€»è®¡: {len(self.articles)} ç¯‡æ–°é—»ï¼ˆå·²å»é‡ï¼‰")
        print(f"æ•°æ®æº:")
        for url in self.urls:
            print(f"  - {url}")
        
        # æŒ‰æ—¶é—´åˆ†ç±»ç»Ÿè®¡
        recent_count = sum(1 for a in self.articles if 'minute' in a['time'].lower())
        hour_count = sum(1 for a in self.articles if 'hour' in a['time'].lower())
        
        if recent_count > 0 or hour_count > 0:
            print(f"\næ—¶é—´åˆ†å¸ƒ:")
            if recent_count > 0:
                print(f"  åˆ†é’Ÿå†…: {recent_count} ç¯‡")
            if hour_count > 0:
                print(f"  å°æ—¶å†…: {hour_count} ç¯‡")
        
        print(f"\næœ€æ–°çš„10ç¯‡æ–°é—»:")
        print("-" * 60)
        for i, article in enumerate(self.articles[:10], 1):
            print(f"{i:2}. {article['title'][:65]}...")
            print(f"    æ—¶é—´: {article['time']}")
            print()


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 65)
    print("Yahoo Finance News å¤šURLçˆ¬è™« - Crawl4AI + Supabaseé›†æˆ")
    print("=" * 65)
    
    # ç¬¬ä¸€æ­¥ï¼šç«‹å³æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
    print("\n=== ç¬¬1æ­¥: ç¯å¢ƒå˜é‡æ£€æŸ¥ ===")
    is_github_actions = os.getenv('GITHUB_ACTIONS') == 'true'
    print(f"è¿è¡Œç¯å¢ƒ: {'GitHub Actions' if is_github_actions else 'Local'}")
    
    # Supabaseé…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
    supabase_config = {
        'url': os.getenv('SUPABASE_URL'),
        'anon_key': os.getenv('SUPABASE_ANON_KEY'),
        'table_name': os.getenv('SUPABASE_TABLE_NAME', 'news_items')
    }
    
    print(f"SUPABASE_URL: {supabase_config['url'] or 'æœªè®¾ç½®'}")
    print(f"SUPABASE_TABLE_NAME: {supabase_config['table_name']}")
    print(f"SUPABASE_ANON_KEY: {'å·²è®¾ç½®' if supabase_config['anon_key'] else 'æœªè®¾ç½®'}")
    if supabase_config['anon_key']:
        print(f"SUPABASE_ANON_KEYé•¿åº¦: {len(supabase_config['anon_key'])}")
        print(f"SUPABASE_ANON_KEYå‰ç¼€: {supabase_config['anon_key'][:20]}...")
    
    # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
    if not supabase_config['url'] or not supabase_config['anon_key']:
        print("\nâŒ ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡:")
        print("   - SUPABASE_URL")
        print("   - SUPABASE_ANON_KEY")
        if is_github_actions:
            print("è¯·åœ¨GitHubä»“åº“è®¾ç½®ä¸­é…ç½®è¿™äº›Secrets")
            exit(1)
        else:
            print("è¯·åœ¨æœ¬åœ°ç¯å¢ƒä¸­è®¾ç½®è¿™äº›å˜é‡")
            exit(1)
    
    # ç¬¬äºŒæ­¥ï¼šç«‹å³æµ‹è¯•Supabaseè¿æ¥
    print("\n=== ç¬¬2æ­¥: Supabaseè¿æ¥æµ‹è¯• ===")
    test_manager = create_supabase_manager(supabase_config)
    if not test_manager or not test_manager.is_connected():
        print("âŒ Supabaseè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        exit(1)
    else:
        print("âœ… Supabaseè¿æ¥æµ‹è¯•æˆåŠŸ")
        # ç«‹å³æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢
        print("\n=== ç¬¬3æ­¥: æ•°æ®åº“æŸ¥è¯¢æµ‹è¯• ===")
        try:
            existing_urls, existing_titles = test_manager.get_existing_articles()
            print(f"âœ… æ•°æ®åº“æŸ¥è¯¢æˆåŠŸ - ç°æœ‰æ–‡ç« : {len(existing_urls)} ä¸ªURL, {len(existing_titles)} ä¸ªæ ‡é¢˜")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
            exit(1)
    
    # åˆ›å»ºçˆ¬è™«ï¼ˆå¸¦Supabaseé…ç½®ï¼‰
    crawler = YahooNewsCrawl4AICrawler(supabase_config=supabase_config)
    
    max_hours = int(os.getenv('INPUT_MAX_HOURS', '2'))  # æ”¯æŒå·¥ä½œæµå‚æ•°
    print(f"\nçˆ¬å–æ—¶é—´èŒƒå›´: æœ€è¿‘{max_hours}å°æ—¶")
    
    # å°è¯•ä½¿ç”¨Crawl4AIçˆ¬å–ï¼ˆç”Ÿäº§ç¯å¢ƒæ— æ–‡ç« æ•°é‡é™åˆ¶ï¼‰
    print("å°è¯•ä½¿ç”¨Crawl4AIçˆ¬å–...")
    articles = await crawler.crawl_with_crawl4ai(max_hours=max_hours)
    
    # å¦‚æœCrawl4AIå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
    if not articles:
        print("\nCrawl4AIçˆ¬å–å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
        articles = await crawler.crawl_fallback(max_hours=max_hours)
    
    if articles:
        # æ˜¾ç¤ºç»“æœ
        crawler.show_summary()
        
        # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
        if crawler.supabase_manager and crawler.supabase_manager.is_connected():
            print("\n=== æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ ===")
            stats = crawler.supabase_manager.get_stats()
            if 'error' not in stats:
                print(f"æ•°æ®åº“æ€»æ–‡ç« æ•°: {stats['total_articles']}")
                print(f"æ•°æ®æ¥æºåˆ†å¸ƒ: {stats['sources']}")
            else:
                print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {stats['error']}")
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆ!")
        print(f"âœ… å¤„ç†æ–‡ç« æ•°: {len(articles)}")
        print(f"ğŸ“Š æ•°æ®å·²ä¿å­˜åˆ°Supabaseæ•°æ®åº“")
        
        # GitHub Actionsç¯å¢ƒä¸‹ä¸ä¿å­˜æœ¬åœ°æ–‡ä»¶
        if not is_github_actions:
            print(f"\nä¿å­˜æœ¬åœ°å¤‡ä»½æ–‡ä»¶...")
            json_file = crawler.save_json()
            csv_file = crawler.save_csv()
            print(f"æœ¬åœ°å¤‡ä»½: JSON: {json_file}, CSV: {csv_file}")
        
    else:
        print("\nâŒ çˆ¬å–å¤±è´¥ï¼Œæ²¡æœ‰è·å–åˆ°æ–°é—»æ•°æ®")
        print("å¯èƒ½åŸå› :")
        print("1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("2. é¡µé¢ç»“æ„å‘ç”Ÿå˜åŒ–") 
        print("3. Crawl4AIé…ç½®é—®é¢˜")
        print("4. Supabaseè¿æ¥é—®é¢˜")
        
        # åœ¨GitHub Actionsä¸­è¿”å›éé›¶é€€å‡ºç 
        if is_github_actions:
            exit(1)


if __name__ == "__main__":
    asyncio.run(main())